import streamlit as st
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS # <-- MUDANÃ‡A 1: Nova importaÃ§Ã£o
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import DirectoryLoader, TextLoader

load_dotenv()

@st.cache_resource
def carregar_recursos():
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        st.error("API Key de Google nÃ£o encontrada. Por favor, configure seus 'secrets'.")
        st.stop()

    print("Carregando documentos...")
    loader = DirectoryLoader('./documentacion/', glob="**/*.md", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"})
    documentos = loader.load()
    if not documentos:
        st.error("NÃ£o foram encontrados documentos na pasta 'documentacion'.")
        st.stop()

    print("Criando embeddings e base de dados FAISS...")
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=api_key)

    # --- MUDANÃ‡A 2: Usando FAISS em vez de Chroma ---
    # A FAISS cria o Ã­ndice em memÃ³ria, o que Ã© perfeito e rÃ¡pido para o Streamlit.
    vector_db = FAISS.from_documents(
        documents=documentos,
        embedding=embeddings
    )
    print("Base de dados FAISS pronta!")

    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", google_api_key=api_key, temperature=0.2)

    retriever = vector_db.as_retriever(search_kwargs={"k": 3})

    prompt_template = """
    Aja como um especialista de suporte tÃ©cnico amigÃ¡vel e prestativo. Sua tarefa Ã© responder Ã  pergunta do usuÃ¡rio baseando-se unicamente nos trechos de documentaÃ§Ã£o fornecidos no contexto.
    InstruÃ§Ãµes:
    1.  Sintetize a informaÃ§Ã£o de todos os trechos de contexto relevantes para formular uma resposta coesa e completa.
    2.  **NÃ£o copie e cole o texto do contexto diretamente.** Explique o processo ou a informaÃ§Ã£o com suas prÃ³prias palavras, de forma clara e natural.
    3.  Se a resposta envolver um passo a passo, organize-a em uma lista numerada ou com marcadores.
    4.  Se a informaÃ§Ã£o nÃ£o estiver no contexto, responda educadamente: "Desculpe, nÃ£o encontrei informaÃ§Ãµes sobre isso em minha base de conhecimento."

    Contexto:
    {context}

    Pergunta do UsuÃ¡rio:
    {question}

    Resposta do Especialista:
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

# --- INTERFAZ DE STREAMLIT ---
st.title("ðŸ¤– Assistente de DocumentaÃ§Ã£o Interna")
st.caption("FaÃ§a perguntas em portuguÃªs sobre a documentaÃ§Ã£o do projeto.")

try:
    chain = carregar_recursos()
except Exception as e:
    st.error(f"Ocorreu um erro ao carregar os recursos: {e}")
    st.stop()

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Qual Ã© a sua pergunta?"):
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.spinner("Analisando documentos..."):
        response = chain.invoke(prompt)
        ai_response = response['result']

    st.session_state.messages.append({"role": "assistant", "content": ai_response})

    st.rerun()
